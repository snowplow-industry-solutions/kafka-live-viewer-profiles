= Snowplow Live Viewer Profile
Paulo Jeronimo <paulo@oso.sh>
:icons: font
:idprefix:
:idseparator: -
:imagesdir: images
:numbered:
:sectanchors:
:source-highlighter: rouge
:toc: left
ifdef::backend-pdf[]
:toc-title!:
:toc: macro
endif::[]
ifdef::backend-html5[]
:nofooter:
endif::[]
// Other attributes
:DatabricksAccelerator: <<databricks-accelerator,Databricks Accelerator>>
:SnowplowMicro: https://docs.snowplow.io/docs/testing-debugging/snowplow-micro/[Snowplow Micro]
:Snowbridge: https://docs.snowplow.io/docs/destinations/forwarding-events/snowbridge/[Snowbridge]
:Kinesis: https://aws.amazon.com/kinesis/[Kinesis]
:Kafka: https://kafka.apache.org/[Kafka]
:KafkaUI: https://github.com/kafbat/kafka-ui[Kafka UI]
:KafkaConnector: https://docs.confluent.io/platform/current/connect/kafka_connectors.html[Kafka Connector]
:DynamoDB: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DynamoDBLocal.DownloadingAndRunning.html[DynamoDB]
:LocalStack: https://www.localstack.cloud/[LocalStack]
:Thymeleaf: https://docs.spring.io/spring-framework/reference/web/webmvc-view/mvc-thymeleaf.html[Thymeleaf]
:WebSockets: https://docs.spring.io/spring-framework/reference/web/websocket.html[WebSockets]

ifdef::backend-pdf[]
[.text-center]
*Author: {author} ({email})* +
*Git commit with doc: {git-commit}* +
*link:README.html[HTML version]*

****
toc::[]
****
endif::[]
ifdef::backend-html5[]
[.text-center]
*link:README.pdf[PDF version]* +
*Git commit with doc: {git-commit}*
endif::[]

<<<
== Current status (as of {docdate})

Right now, this project can work my machine (running all inside a Windows/WSL2 environment) without worrying about setting up an infrastructure to run everything as a {DatabricksAccelerator}, as requested in link:requirements.pdf[the document]. But, this will be my next step (<<databricks-setup>>) and will be delivered soon (<<question-databricks,question here>>).

=== What is already in operation at this point?

A sequence of steps, defined below, to permit you to run Snowplow, via Docker Compose, sending data to Kafka through via {SnowplowMicro} and {Snowbridge}.

These steps demonstrate how to test, locally, the sending of events https://snowplow-incubator.github.io/snowplow-javascript-tracker-examples/media/[from this application] (but <<step1,running it locally>>) until its consumption, via {KafkaConnector}, by an application written in Java 21.

=== Limitations

. [[limitation1]] In this version, as I am using {SnowplowMicro} to generate events, and it does not have native support for generating events for {Kinesis}, this is not addressed.

<<<
=== TODO or [line-through]#DONE or CANCELED#)

. Create a entire Docker Compose infrastructure to run components locally.
.. [line-through]#Configure a {SnowplowMicro} container#. *<- DONE*
.. [[snowbridge-caller]] [line-through]#Create a customized image to call {Snowbridge} and send events to it by reading data from a micro.tsv file#. *<- DONE* *<- snowbridge-caller*
.. [line-through]#Configure a Node.js container to run the media app#. *<- DONE*
.. [line-through]#Configure the {Snowbridge} container to sent events to Kafka.# *<- DONE*
.. [line-through]#Configure a {Kafka} container#. *<- DONE*
.. [line-through]#Configure a {KafkaUI} container#. *<- DONE*
.. Configure a {DynamoDB} container.
.. [line-through]#Create a Java 21 application to consume messages from Kafka#. *<- DONE*
... [line-through]#Consume messages from Kafka (coming from snowbridge)#. *<- DONE*
... Record messages received to {DynamoDB}.
... [line-through]#Determine a state from a viewer using a video state machine#. *<- DONE*
.. [line-through]#Create a Docker image to run the Java app#. *<- DONE*
. [line-through]#Create and keep this documentation updated#. *<- DONE*
. Maybe:
.. Refactor the code to make it cleaner and create more test code.
.. [line-through]#Separate the frontend code from the Java backend#. *<- DONE*
... So the Spring backend will skip to use {Thymeleaf} and be more focused on manage backend messages (Kafka and {WebSockets}).
... Since it is only a static page, it can be in a separate container managed by Ngnix (as an example).
.. [line-through]#frontend -> Detect if WebSocket Server (backend) is up or down#. *<- DONE*
.. [line-through]#frontend -> Change to use Snowplow's website colors#. *<- DONE*
. Configure Snowplow to send messages to {Kinesis} and test it locally with {LocalStack}. *<- <<question-localstack,question here>>*
. [[databricks-setup]] [line-through]#Create a {DatabricksAccelerator}. *<-<<question-databricks,question here>>#* *<- CANCELED*

<<<
.My questions (doubts):
****
[[question-localstack]] question-localstack) Do we need to create a version of this project to run with {LocalStack}?::
Reason: this is a way to insert {Kinesis} in this solution and continue to test it locally. Currenty, to skip <<limitation1,this limitation>> and use only Snowplow Micro in this version, I created the <<snowbridge-caller>> project. +
https://osodevops.slack.com/archives/C07RAQVAAJH/p1731493555873649?thread_ts=1731453220.008699&cid=C07RAQVAAJH[This was one of the things I chatted with Trent].

[[question-databricks]] question-databricks) Do we need to create a version of this project to run inside Databricks (as a real <<databricks-accelerator>>)?::
****

<<<
== Steps (to run this application as is)
:numbered!:

[[step0]]
=== Step 0 -> Prerequisites

. Start a Ubuntu Linux (it can be running on a WSL2 environment) terminal.
. Make sure you have docker (and docker compose) installed.
. Clone this project with Git and cd to it.
+
[[github]]
[,console]
----
$ repo=git@github.com:osodevops/snowplow-live-viewer-profile-generator.git
$ git clone $repo && cd $(basename $repo .git)
----

NOTE: You don't need Java or Node.js configured on your machine to follow the steps below.

[[step1]]
=== Step 1 -> Start the containers

[,console]
----
$ ./up.sh
----

[[step2]]
=== Step 2 -> Open http://localhost:3000 to generate the events

After open this link, configure the collector endpoint:

image:js-tracker-1.png[]

Open the "Custom media tracking demo":

image:js-tracker-2.png[]

You will get a page like this one:

image:js-tracker-3.png[]

[[step3]]
=== Step 3 -> Open http://localhost:8280 to see the "Snowplow Live Viewer Profile" UI.

See details on the <<video2>>.

[[step4]]
=== Step 4 -> (optional) Open http://localhost:9090/micro/ui to watch events

You will get a page like this one:

image:micro-ui.png[]

[[step5]]
=== Step 5 -> (optional) Open a terminal to watch events sent by snowbridge

To watch the number of events sent by snowbridge, type:

[,console]
----
$ ./data/snowbridge.watch.sh
----

[[step6]]
=== Step 6 -> (optional) Open http://localhost:8080 to see the events exported to {KafkaUI}.

See details on the <<video1>>.

[[step7]]
=== Step 7 -> Stop the containers

To stop all the containers:

[,console]
----
$ ./down.sh
----

[[step8]]
=== Step 8 -> Restart this lab (to run it from scratch again)

To restart this lab:

[,console]
----
$ ./restart.sh
----

[WARNING]
.Warnings:
====
. Make sure you call the script `down.sh` before calling `restart.sh`.
. The script `restart.sh` will call the script `clean.sh` as its firts step.
. The script `clean.sh` will destroy any data generated by these containers.
====

:numbered:
<<<
== References

. [[databricks-accelerator]] *databricks-acelerator*:
.. https://github.com/databricks-industry-solutions/
.. https://www.databricks.com/solutions/accelerators

== Videos demonstrating the with status of this project

* [[video2]] *video2 ->* https://www.youtube.com/watch?v=CZ5gGOPkGtY -> Published on YouTube (unlisted) on Nov 18, 2024.
* [[video1]] *video1 ->* https://www.youtube.com/watch?v=94U1-Ryjv20 -> Published on YouTube (unlisted) on Nov 11, 2024.

<<<
== What I learned in the space of two weeks

Since beginning this project on November 4 and working with Snowplow for about two weeks (by November 18), I've gained some insight into the tool. I've discovered that it's incredibly useful for collecting behavioral data from applications running across various environments. Before diving into the solution requested by Snowplow (as documented here), I decided to experiment with it on a personal project: https://github.com/paulojeronimo/venom-bot1[a WhatsApp chatbot I built in JavaScript]. I plan to write an article about that experience soon.

As for this web application, creating this solution gave me a good initial insight into the power that Snowplow offers. It was not easy, however, to set up a local structure to make it work only on the machine. However, I have good experience with Docker, Docker Compose, Bash, JavaScript, and Java. All of this, added to the documentation provided by Snowplow, helped me build this solution.

Initially, I was particularly intrigued by the fact that Snowbridge did not always produce the same output every time I gave it https://github.com/osodevops/snowplow-live-viewer-profile-generator/blob/main/data/samples/micro.1.tsv[an input file] and asked it to generate the events (in stdout) according to https://github.com/osodevops/snowplow-live-viewer-profile-generator/blob/main/labs/snowbridge-transform/script.6.js[the transformation I configured]. The events generated by it, in this output, were out of order, and, in my head, https://github.com/osodevops/snowplow-live-viewer-profile-generator/blob/main/labs/snowbridge-transform/README.adoc?plain=1#L71[a warning] light went off that this would be a problem for processing the state that a viewer would be in. I spent a good amount of time developing https://github.com/osodevops/snowplow-live-viewer-profile-generator/tree/main/labs/snowbridge-transform[a lab to understand this]. Apparently (I haven't investigated it yet) this should be implemented by implementing multiple threads within Snowbridge. But, to avoid spending too much time trying to understand this, I https://github.com/osodevops/snowplow-live-viewer-profile-generator/blob/13a49d332c9c3d5fabdbf4fd195fbed99eed48f2/compose.yaml#L19[reduced its call time in snobridge-caller by just 1 second]. Then, the result was exactly what I expected.

So far, learning from Snowplow has been a lot of fun. I hope to learn even more about it so I can continue to help your team with whatever they need.

From now on I will focus on the implementation that will save the records from Kafka to DynamoDB. I will give more feedback when I finish this.

<<<
== About this document

This document is written in AsciiDoc format.
Its source code is the file `README.adoc` (inside the GitHub repo of <<github,the project>>).

The script `README.sh` generates the files `README.html` and `README.pdf`.
